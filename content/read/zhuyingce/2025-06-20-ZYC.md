---
title: 人类决策过程中的量子强化学习
author: 朱英策
date: 2025-06-20
showDate: true
showTOC: true
---
# 论文信息
Li, J. A., Dong, D., Wei, Z., Liu, Y., Pan, Y., Nori, F., & Zhang, X. (2020). Quantum reinforcement learning during human decision-making. *Nature human behaviour*, *4*(3), 294–307. https://doi.org/10.1038/s41562-019-0804-2

# [[论文原文](../Source_Files/Quantum reinforcement learning during human decision-making.pdf))]
# 关键词
经典强化学习、量子强化学习
# 摘要
经典强化学习已被广泛应用于神经科学和心理学中，量子强化学习在计算机模拟中表现出更优越的性能，却从未在关于人类决策的实证研究中得到检验。本研究考察了 QRL 是否能够合理解释基于价值的决策过程。我们使用来自健康个体与吸烟者完成爱荷华赌博任务的行为数据和功能性磁共振成像（fMRI）数据，对 2 个 QRL 模型和 12 个 CRL 模型进行了比较。结果显示，在所有受试者群体中，QRL 模型相较于表现最佳的 CRL 模型也表现良好，并进一步揭示了与量子态相关的内部变量在健康个体和吸烟者的内侧额叶回中均有表现。这一发现表明，基于价值的决策过程在行为层面和神经层面上都可以通过 QRL 进行解释。

# 3.结果

## 3.1任务设计与被试表现

招募了健康被试（对照组）和吸烟被试（吸烟组）在功能性磁共振成像（fMRI）扫描仪中完成爱荷华赌博任务（IGT）。在该任务中，被试在每轮选择四副牌中的一副，以获得潜在的奖励或惩罚积分。他们需要学习每副牌的优劣，以最大化总奖励。无论所属组别如何，被试在最后20轮中选择优质牌组的次数均显著高于前20轮，表明大多数被试逐渐掌握了任务。

## 3.2QSPP模型表现优异

12个经典强化学习（CRL）模型旨在将爱荷华赌博任务（IGT）的表现分解为若干子成分，大致而言，这些模型都包含三个阶段：

评估结果（例如，价值加坚持性规则（VPP））；

更新期望值（例如，衰减学习规则）；

做出选择（例如，与试次无关的选择规则（TIC））。

基于量子强化学习（QRL）的原理，我们开发了另外两个QRL模型：量子叠加状态学习模型（QSL）和量子叠加状态加坚持性模型（QSPP），通过将经典强化学习（CRL）模型中的每个经典成分替换为其量子对应物。提出了量子强化学习（QRL）框架中主要算符的几何和代数解释.

基于是否包含坚持性部分，带有价值加坚持性规则（VPP规则）的模型（VPP模型）和量子叠加状态加坚持性模型（QSPP模型）被视为双项模型，而其他模型则被视为单项模型。

模型通过优化算法进行拟合，目的是最大化每位被试选择序列的对数似然（LL）。为了验证量子强化学习（QRL）模型的可行性，我们基于拟合优度标准和模拟方法进行了模型比较.

关于拟合优度标准：修正后的AICc和BIC提供了对一步预测的直接评估。AICc或BIC值越小，表示模型拟合效果越好。

图3：

a：健康被试的AICc值；b：吸烟组的AICc值；c：504名健康被试的超级组的AICc值：

所有模型的表现均优于基线模型（该模型假设智能体以固定概率选择每副牌）。在吸烟组和超级组中，量子叠加状态学习模型（QSL）比所有单项经典强化学习（CRL）模型拟合效果更好，在对照组中则与表现最佳的单项CRL模型PVLDecayTDC相当。量子叠加状态加坚持性模型（QSPP）在三组中均优于所有其他CRL模型，包括以往研究中被认为拟合效果最佳的VPPDeltaTIC模型。

d：健康被试的BIC值；e：吸烟组的BIC值；f:504名健康被试的超级组的BIC值：

与AICc值的结果类似。

图四：基于变分贝叶斯方法进行了贝叶斯模型比较，将模型视为随机变量，并估计模型空间上的分布。基于AICc和BIC，我们得到了期望模型似然，结果一致。QSPP模型显示出的推断频率高于所有其他CRL模型。两个QRL模型联合提供的超越概率在所有情况下均大于0.99。

QSPP模型在行为层面上与经典强化学习（CRL）模型相当，甚至表现更优，证明了量子强化学习（QRL）是描述基于价值的决策行为的强大框架，在后续分析中，主要集中于QSPP模型和拟合效果最佳的VPPDecayTIC模型，分别作为QRL和CRL模型的代表。

基于模型的功能磁共振成像（fMRI）分析，该方法能够将模型预测与fMRI数据关联起来，从而定位大脑中假设的决策过程，并区分竞争性的假设。我们基于两组在结果阶段的血氧水平依赖信号（BOLD数据）分析了广义量子距离。在对照组中，广义量子距离显著激活了内侧额回/前扣带皮层（MeFG/ACC）、中央前回、岛叶、楔前叶、左侧梭状回、下顶叶小叶（IPL）、中额回（MiFG）以及右侧下颞回/梭状回，这显示出一种独特的量子类神经机制，揭示了外部信息如何改变内部状态。此外，我们在吸烟组未发现与广义量子距离相关的激活，表明吸烟者可能存在这种表征的损伤。

## 3.3QSPP模型揭示的不确定性相关神经基础

不确定性可能是另一个重要变量，有助于更好地理解VPPDecayTIC模型与QSPP模型所反映的学习过程差异。内部状态的不确定性是一种重要的任务相关变量，在神经层面有所体现，它反映了外部环境的波动性，与结果相互作用，并辅助学习。计算了两种模型所提供的不确定性。结果显示不确定性随时间下降，意味着被试对任务规则的不确定性逐渐减少。

基于模型的fMRI分析，研究了不确定性及其与惩罚/奖励的交互作用，基于BOLD数据：在对照组中，不确定性与惩罚交互作用效应的差异（对照组QSPP模型减去对照组VPPDecayTIC模型）在右侧内侧额回（MeFG）、左侧眶额皮层（OFC）、左侧内侧额回/前扣带皮层（MeFG/ACC）、中颞回（MiTG）以及左侧中颞回/角回/后扣带回（PCC）/楔前叶显著激活，这些结果揭示了学习过程中，惩罚效应如何被不确定性调节的独特量子类机制。在吸烟组中，我们同样发现了交互作用效应差异（吸烟组QSPP模型减去吸烟组VPPDecayTIC模型）与左侧内侧额回、左侧中颞回和左侧下顶叶小叶（IPL）呈正相关。

对于不确定性与奖励的交互作用，我们在对照组发现了左侧内侧额回（接近显著）、左侧中颞回以及左侧中颞回/角回的激活。

# 4.讨论

在本研究中，我们为IGT开发了另外两个量子强化学习（QRL）模型。我们将这两个QRL模型与12个经典强化学习（CRL）模型进行了比较，发现QSPP模型的表现与最佳的CRL模型相当。基于fMRI数据，研究了健康被试和吸烟者的大脑中广义量子距离以及不确定性与惩罚/奖励的交互作用。结果显示对照组和吸烟组在脑活动上既存在若干一致性，也有明显差异。

QSPP模型在三个组别中均表现出优越性能，表明该模型具有较好的泛化能力，适用于更多样化的被试群体。Delta规则在长期（整个序列）预测中表现更好，而Decay规则在短期（一步预测）中表现更佳。量子强化学习（QRL）模型在拟合优度标准和模拟方法中均表现良好，表明它们在长短期预测中都具有较好的准确性。

价值型决策中的学习过程就是对每个动作权重的调整（比如对每副牌的偏好/倾向）。在经典强化学习（CRL）模型中，权重通过价值函数来实现（即对各牌堆的期望值），而在量子强化学习（QRL）模型中，权重则由每个动作的概率振幅表示（整体的叠加态）。认知上，在每次试验的学习阶段，CRL模型会更新所选动作的权重（例如通过预测误差），而未选动作的权重则会衰减或保持不变，且这种调整是相互独立的，不受所选动作影响。然而，在QRL模型中，叠加态可以被可视化为三维球面（Bloch球面）上的一个点（向量），每个动作权重的调整同时反映在该点位置的移动上，这一移动通过围绕某一轴的旋转来实现。

fMRI研究支持了量子认知在神经层面的观点：量子距离衡量两个量子态之间的可区分性（即它们的接近程度），从而描述了通过结果学习对内部状态的影响。广义量子距离考虑了量子距离与跃迁幅度之间的对偶性，因此能够捕捉主算符的整体特性。广义量子距离可以被视为内部叠加态与结果之间的相互作用，其核心概念与不确定性与结果的相互作用类似。广义量子距离在中前额叶回/前扣带皮层（MeFG/ACC）、中央前回、岛叶、中额叶回、楔前叶、下顶叶小叶（IPL）和梭状回中都有表达。中额叶回、楔前叶和梭状回在许多涉及不确定性的决策任务中被报道激活。下顶叶小叶被认为在不确定性下的决策中发挥作用，负责更新对动作的评估。中央前回与决策中的运动反应准备和执行有关。岛叶涉及内感受、情绪和风险决策，如风险体验表示和预测误差表示。

不确定性是一种反映外部环境的内部状态属性，受到结果（惩罚和奖励）的影响。因此，类似于广义量子距离，不确定性与惩罚/奖励的交互揭示了结果对内部状态的独特量子类调节效应，涉及整合与学习。在对照组和吸烟组中都再次发现同一脑区——中前额叶回（MeFG）的激活，表明MeFG中的量子类机制可能是价值决策中量子类效应的枢纽，影响强化学习过程中众所周知的认知过程。不确定性与惩罚的交互激活了前扣带回（ACC）、眶额皮质（OFC）、中颞回（MiTG）、角回/中颞回和后扣带回（PCC）/楔前叶，不确定性与奖励的交互激活了中颞回和角回/中颞回。ACC代表预测误差和学习率，并监控和整合结果。OFC被认为代表预测误差、更新价值、评估不确定性程度、优化不确定性下的学习率，并代表外部结果的抽象特征。角回与需要视觉注意的风险决策相关。PCC据报道在伊戈诺任务（IGT）中与MeFG、楔前叶、中额叶回、梭状回和前中央回具有功能连接，而这些脑区也被广义量子距离激活。

量子强化学习（QRL）框架在决策神经科学和神经经济学领域具有潜在应用价值，并且它有望成为经典强化学习（CRL）的有力竞争者。鉴于CRL在情绪研究、精神疾病如成瘾和抑郁、社会行为、自由意志及许多其他认知功能方面取得的巨大成功，希望QRL能为这些领域带来新的启示。对QRL框架进行更广泛、更深入的研究是十分必要的。QRL属于量子学习的一种，我们也期待未来能探索包括量子贝叶斯学习和量子神经网络学习在内的其他方法。随着QRL与决策神经科学的结合，可能形成一个新的学科——量子神经经济学。
