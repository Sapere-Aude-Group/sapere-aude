---
title: 机器人辅助治疗自闭症儿童的3D人感、动作和情绪识别
author: 范穗穗
date: 2021-02-28
showDate: true
showTOC: true
---
文献:Marinoiu, E., Zanfir, M., Olaru, V., & Sminchisescu, C. (2018). 3D Human Sensing, Action and Emotion Recognition in Robot Assisted Therapy of Children with Autism. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. https://doi.org/10.1109/cvpr.2018.00230 
[论文原文](../Source_Files/2021-02-28-FSS2.Pdf)
# 1.背景介绍
 自闭症患者难以使用和理解语言和非语言交流，难以识别和正确地反应他人的感受，也无法对来自他人的社会和情绪迹象作出语言或非语言反应。然而，自闭症患者可以很好地应对基于规则的可预测系统，例如计算机。最近的发展表明，使用类人型机器人进行心理教育治疗具有优势，因为自闭症儿童在这种机器人周围的感觉比在人在场时感觉更舒适。虽然具有面部表情的类人机器人可以帮助提高自闭症儿童识别他人情绪的能力，但大多数研究都是基于遥控人机互动（remote controlled human-robot interaction，HRI）。在本文中，研究者介绍了在非分阶段视频上定义的细粒度动作分类和情绪预测任务，这些任务是在自闭症儿童的机器人辅助治疗期间录制的。分析了包含儿童治疗师互动和微妙的行为注释的大规模视频数据集。建立了几个动作和情绪识别基线，包括基于儿童表征的系统以及共同捕获儿童和治疗师的模型。数据，注释和识别模型可从http://vision.imar.ro/de-enigma在线获得。在这里，研究者提出了一种仅使用3d骨架数据在效价空间中进行连续情绪识别的自动化方法。
# 2.DE-ENIGMA操作注释设置
DE-ENIGMA数据集包含自闭症儿童治疗过程的多模式记录。在机器人辅助的会话中，一个孩子和一个治疗师坐在放置机器人的桌子前。治疗师远程控制机器人，并使用它来使孩子参与学习情绪的过程。课程包括一个“自由游戏”部分（孩子在玩耍中自己选择的玩具）和一个实际的治疗部分。该疗法基于以下场景：治疗师会展示卡片，这些卡片描述了机器人也会复制的各种情绪（快乐，悲伤，愤怒等），并且孩子必须将情绪与所执行的情绪进行匹配。
![图1](../Supporting_Information/2021-02-28-FSS2-Fig1.png)
# 3.过程
## 3.1记录：
对来自7个孩子的多次治疗课中精选的视频进行了标注，其中包含37个动作类别。
## 3.2标注过程：
总共标注了3757个序列，平均持续时间为2.1秒。治疗视频的标注依赖于研究者开发的基于网络的工具，该工具可以（i）选择时间范围，并且（ii）为它们分配类别标签。有约三分之一的动作是互动序列（如，指向治疗师，转向治疗师）。
![图2](../Supporting_Information/2021-02-28-FSS2-Fig2.png)
![图3](../Supporting_Information/2021-02-28-FSS2-Fig3.png)
儿童行为在标注序列的数量上有很大不同，某些行为的活跃度明显低于其他行为。
## 3.3从RGB数据重建骨架：
DMHS是一个多任务深度神经网络，可估计2d和3d关节位置以及人的语义人体部位标签。为了改善部分可见人的基于DMHS的3d姿势估计，研究者收集了在自然图像中经常可见的那些人类关键点配置的统计信息。使用来自COCO培训（关键点挑战）的带有2d关节注释的图像，并选择50个最常见的配置来创建基于Human80k的新数据集（H80KPartial）。接下来，微调H80KPartial上的语义分割，并将其用作初始化网络的3D姿态估计任务。
## 3.4参数化人类模型推理：
依靠前馈-反馈（feedforward-feedback）模型，将DMHSPV的人体检测，2d和3d姿态预测与基于SMPL人体表示的基于形状的体积细化相结合。首先将姿势外观从DMHSPV转移到SMPL，然后使用此配置作为语义图像拟合的初始化。
![图4](../Supporting_Information/2021-02-28-FSS2-Fig4.png)
## 3.5基于骨架的动作分类：
二维姿势特征：用于二维姿势估计的最新方法具有良好的准确性和速度。但是，仅使用2D人体关节位置来解释孩子的动作可能是不够的，因为深度信息对于消除不同动作可能至关重要。
三维姿势特征：我们考虑从DMHSPV获得的3d人体骨骼，单帧SMPL模型推断DMHS-SMPL-F和时间平滑推断DMHS-SMPL-T。
交互建模：每个姿势特征和模型对都有其自己的治疗师姿势重建（reconstruction）的集成方法。
移动姿势：我们的基准之一是基于框架的，该框架在帧级3d姿态描述符上使用KNN分类器。通过将3d骨架位置与在5帧窗口上计算出的关节的速度和加速度进行级联来构建描述符。在我们的工作中，我们仅考虑静态姿势和速度分量。
卷积（Convolutional）神经网络：考虑了儿童和治疗师的3D姿势特征。为避免过度拟合，我们向每个训练序列添加随机旋转（围绕Y轴±15°）。为CNN提供通过Kinect获得的3d骨架特征，以及DMHS-SMPL-T，后者是Moving Pose框架中性能最好的RGB模型。
递归（Recurrent）神经网络：该模型由5个双向递归网络的层次结构组成，每个递归网络都接收5个骨骼子组件的关节作为输入：躯干，左臂，右臂，左腿和右腿。在随后的层中，子组件网络学习到的表示被分层融合，并作为输入提供给上层。使用Kinect 3d姿态估计以及DMHS-SMPL-T推断的姿态估计来测试网络。Kinect的动作分类准确率为37.8％，DMHS-SMPL-T的动作分类准确率为36.2％。
# 4.持续情绪预测
与使用一些分类的情绪类别相比，在连续的空间中表达情绪可以捕获更多的微妙的情绪状态。
# 5.总结
本研究引入了大规模细粒度的动作和情绪识别任务，这些任务是在自闭症儿童的机器人辅助治疗会议期间录制的非分阶段视频上定义的。研究了将前馈-反馈组件相结合的最新RGB 3d人体姿势重建方法如何适应该问题，并基于儿童和治疗师的2d和3d表示评估了多个动作和情绪识别基线。结果表明，经过适当调整的当前RGB数据2d和3d重建方法与工业级RGB-D Kinect系统具有竞争力。