---
title: 加奶的茶？顺序事件理解的等级生成框架
author: 王萌
date: 2021-08-22
showDate: true
showToc: true
---

文献：Kuperberg, G. R. (2020). Tea With Milk? A Hierarchical Generative Framework of Sequential Event Comprehension. *Topics in Cognitive Science, 13*(1), 256–298. doi:10.1111/tops.12518

[原文](../Source_Files/2021-08-22-WM1.pdf)

# 1.背景和目的

为了让周围的事物有意义，我们需要不断将感官输入细分到离散事件中，本文提出一个分层生成模型（hierarchical generative models）。通过为即将到来的事件生成概率预测，生成模型确保我们能够跟上感知到的投入展开的快速速度。通过跟踪我们对其他代理目标的确定性以及多个时间尺度上的预词错误量级，生成模型使我们能够通过推断目标何时改变来检测事件边界。此外，通过灵活适应环境的更广泛动态和我们自己的理解目标，生成模型使我们能够以最佳方式分配有限的资源。这种分层生成框架工作为人类大脑如何如此毫不费力地处理事件提供了新的见解，同时高度照亮了事件理解、生产和学习之间的基本联系。

# 2.事件表征

## 2.1 事件的核心属性
 
什么是事件？不同领域的解释不同，但是对他的核心特征和功能属性有一些共识。

1. 由多种不同元素组成。

2. 传达了世界状态的变化。

3. 大多数真实世界的事件持续时间有限。虽然有些事件比其他事件持续的时间更长，但几乎所有事件最终都会结束，这些结束都以结束状态为标志。

4. 本质上是动态的。

## 2.2 事件序列

 “The woman in the kitchen made herself a cup of tea and then cleaned the fridge,”是一个事件序列，<woman made self tea> and <woman cleaned fridge>是两个独立事件。

为了将这些事件联系在一起，我们利用我们对人类主体如何对其环境采取行动的基本知识，考虑到我们自己的身体、周围空间以及我们周围物体的可供性和功能的限制。利用已有知识，结合已发生的事件来决定未来将发生什么。即使是单个事件的结束状态也可以作为前提条件，限制事件发生的可能性。

## 2.3 事件模式

我们还根据我们的长期语义知识来描述事件，这些知识可以推断和概括我们在许多遇到类似事件时的经验。

有无数可能的单个事件可以落入特定模式，以及在给定模式中对这些事件进行排序的多种不同方式。给定序列中的任何单个事件都不能准确地告诉我们该序列是关于什么的。子事件可能属于多个事件模式的部分。因此，与其将事件模式表示为固定的、结晶的记忆结构，它们必须按概率编码，以便在给定模式内最有可能发生的事件和事件序列在表示空间中聚集在一起，但没有单个事件或事件序列绑定到任何给定的集群。此外，模式相关事件的集群必须可用于在任何给定情况下动态组装和拆卸，以便我们可以根据需要混合和匹配。

# 3.事件理解的挑战

为了理解实时展开的视觉事件，我们必须能够快速调动上述大量事件知识，并使用它来理解其他代理产生的连续动作。考虑到每个单独事件在我们眼前展开的速度以及女性目标变化的频率，事件理解提出了巨大的计算挑战。

## 3.1 预测概率

事件理解期间预测的主要优势是速度和准确性。感知信息以非常快的速度展开。此外，它有时是模棱两可的或不完整的。通过提前预测，我们可以利用我们的先验知识来填补任何空白并保持领先于输入的一步。

大多数事件理解框架都假设这些预测是由事件模型隐式生成的——在工作记忆中处于活动状态的先前上下文（迄今为止观察到的事件序列）的高级表示。通常还假设事件模型生成的预测进一步受到工作记忆中与模式相关知识的限制。一些模型还提出，这些隐式预测会主动向下传播到较低级别的表示，在那里它们在这些较低级别预先激活即将到来的信息。只要这些预测是概率性的并且基于反映输入本身统计结构的内部表示（在这种情况下，事件模型和与模式相关的事件集群），那么它们应该增加理解的速度和准确性。

相关ERP研究证据：相比不可预测事件，可预测的事件更容易处理，并且在事件发生后 300 到 500 毫秒内产生更少的诱发神经活动——更小的 N400 反应。

## 3.2 检测环境变化的挑战：事件边界

因为概率预测只有在反映输入本身的统计结构时才是最佳的，所以为了继续有效地预测，我们能够检测环境统计结构的系统变化是至关重要的。比如，如果厨房里的女人开始清理冰箱，继续预测与茶有关的事件显然会适得其反。通过快速检测此类变化的证据，我们可以脱离我们原来的 {Woman make her her own a cup of tea} 事件模型，开始构建一个新的 {Woman cleans the glass} 事件模型。

在我们环境的统计结构中检测这种类型的系统变化——即检测视觉事件序列之间的边界——并非易事。因为哪些特定事件或事件子序列可以属于任何给定的事件模型及其关联的模式，存在显着的可变性。如果我们能够将所有事件作为一个批次一起观察，那么检测事件模型之间的边界将不那么具有挑战性。这是因为事件模型部分地由其组件事件之间的完整关系集定义。然而，在实时理解过程中，事件会随着时间的推移依次可用。因此，为了继续有效地预测，我们需要能够根据有限的信息推断事件边界。

## 3.3 作为环境变化指标的预测误差

我们如何能够在实时理解期间检测事件序列之间的边界的一项提议来自事件分割理论。根据这个理论，我们跟踪每个传入事件产生的隐式预测误差的大小——该事件与当前事件模型的状态不一致的程度。如果我们遇到与我们隐式预测的事件非常不同的事件，由此产生的“预测错误”会导致我们通过脱离旧的事件模型及其关联的与模式相关的事件簇来更新工作记忆的内容，并且切换到新的事件模型。

紧接在事件边界之后发生的事件通常被认为比发生在两个边界中间的事件更难预测。如果，当我们看着女人给自己泡一杯茶时，突然看到她抓起一块海绵，这个事件会产生很大的预测误差。

## 3.4 开放性问题

1. 如何在事件理解期间最初构建事件模型？如何知道要从长期记忆中检索哪些与模式相关的集群来构建新的 {Make self a cup of tea} 事件模型？

2. 如果我们遇到一个产生很大预测误差的事件，我们如何知道这个误差是否大到足以脱离我们当前的事件模型并开始构建一个新的事件模型？即使输入似乎违反了先前的预测，我们有时如何能够避免脱离当前的事件模型？如果一个大的预测错误确实导致我们检测到事件边界，为什么以及如何驱使我们脱离当前的事件模型，从长期记忆中检索新的模式相关信息，并切换到新的事件模型?

3. 大预测误差的检测是事件边界的唯一指标吗？当我们最不确定接下来会看到什么时，我们会密切关注事件流中即将发生的事件。在我们真正遇到下一个不可预测的事件之前，我们是否可以利用事件流中的这种不确定性来脱离当前事件模型及其关联的与模式相关的事件集群？

# 4.分层生成框架

## 4.1 生成模型：结构和原理

框架的核心是生成模型。模型包括一个较低的层次，每个单元代表一个单一的观察，以及一个更高的层次，编码这些观察的全部可能原因。更高级别的信息以更抽象的形式进行编码，该形式捕获了在较低级别编码的信息的高阶限制。

我们可以使用这个生成模型来推断最可能的一组原因，这些原因导致任何给定的观察结果或环境中的一组观察结果。每个可能的原因是一个假说，假说表示的是信念的程度，完整的信念集是概率分布的，所有信念总和为1.初始信念可以转变为一组新的信念，这个过程是信念更新。

多个生成模型可以以层次结构的方式链接在一起，在层次结构的更高层次上的表示在依次更高的抽象层次上编码信息。在这种类型的层次生成模型中，信念更新在层次结构的多个层次上进行，在一个层次上推断出的原因作为上一层的观察结果。通过多个信念更新周期，模型原则上应该确定最能解释输入统计结构的假设组合。

本文提出一个三级分层生成模型，它可能代表我们关于其他代理如何产生现实世界视觉事件序列的概率假设，以及我们如何反转这个模型理解这些输入。

### 4.1.1 从顺序事件推断事件集群

为了将随时间顺序展开的视觉事件联系起来，生成模型需要编码我们关于其他主体如何与周围环境交互以产生这些事件的基本知识。基本知识是会随时间相互作用的，所以生成模型必须是动态的。

在这种动态概率生成模型中，推理必然需要一个信念更新和概率预测的迭代过程。新事件在第一层被观察到，输入到第二层，今儿更新事件集群。新的事件流预测下一个事件。这种类型的预测是隐式的：在任何给定时间，推断出的事件集群固有地限制了接下来可能发生的概率空间。这是由模型的动态特性决定的。当观察到下一个事件时，事件簇将再次更新。每个传入事件引起的变化可以概念化为由这个新事件引起的概率分布的变化，它可以被认为是一种隐含的预测错误。反过来，新推断的事件集群隐式预测其下一个状态，并观察到另一个新事件。因此，通过隐式概率预测和更新的迭代循环，这个动态概率生成模型应该增量地推断最有可能产生观察到的单个事件的完整序列的事件集群。

### 4.1.2 从事件集群推断其他代理的目标

根据已有知识推断一个事件集群，但如果出现一个不可预测事件，那么会出现很大的隐含预测差。无法和先验知识相结合，需要开始构建一个新的事件集群。

该模型目前缺少的基本假设是，主体会产生一系列动作来满足他们的长期目标，而且这些目标可以改变。目标会驱使主体生成一系列事件，因此，要真正理解这个动作序列（推断其潜在的潜在原因并解释输入），我们必须能够推断主体的目标。为了适应这个基本假设，我们需要在我们的生成模型中添加第三个层次，在这个层次中，我们表示我们对女性在厨房活动时可能考虑的目标范围的信念。这些目标中的每一个都将以抽象形式表示，这将需要捕获关于目标如何驱动代理产生事件序列的两个进一步假设。

这三个层次共同构成了一个层次化的生成模型。推理在模型的最高（第三）和中间（第二）级别进行，因为每个传入事件依次可用于最低（第一）级别。在任何给定时间，模型的第二层推断最有可能产生迄今为止观察到的单个事件序列的事件集群——当前事件模型——而模型的第三层推断最有可能产生的目标生成在第二级推断的事件模型。

![图1](../Supporting_Information/2021-08-22-WM1-fig1.png)

## 4.2 分层动态预测编码

作为实时事件理解期间的推理引擎，我们需要一种算法，该算法可以通过以自上而下和自下而上的方式将信息从皮层层次结构的一个级别传递到另一个级别来执行（或近似）迭代置信度更新。

预测编码不仅在理论上是生成性的；它是主动生成的。 在较高级别（级别 2）表示的信息通过反馈连接主动向下传播，以重建较低级别的活动。从新环境输入实际出现时诱导的状态中减去较低级别（级别 1）的重建活动，并且只有活动的差异（观察到的 - 预测）通过以下方式传递回较高级别（级别 2）前馈连接。我将新的自下而上输入与第一级输入的自上而下重构之间的差异称为第一级自下而上预测误差。当这个自下而上预测误差达到更高级别（第二级)，它会引起其状态的变化。从旧状态到新状态的转变是隐含的预测误差。随着环境中的每个新输入变得可用，重复此过程直到自下而上预测误差的幅度最小化。这样，算法要么近似，要么在某些假设下执行贝叶斯推理。

## 4.3将计算原理映射到认知表征和过程

大多数事件理解的心理模型都诉诸于这样一种观点，即正在进行的解释——事件模型——在工作记忆中以活动状态表示。工作记忆的活跃状态是第二层的当前活动状态。事件模型本身就是动态演化的事件在任何给定时间推断的集群。它反映了我们对已观察到的、当前正在观察的以及我们认为将在给定的事件序列中观察到的内容的理解。

在任何给定时间，事件模型生成的隐式预测代表（a）给定我们的空间知识和周围物体的可供性的一组可能和可能的即将发生的事件，以及（b）基于我们对当前目标的信念，已激活/检索的一组与模式相关的事件集群。

在任何给定时间，在生成层次结构的第二层表示的工作记忆包括当前事件模型及其对未来状态的隐式概率预测。这些隐含的预测取决于我们关于我们如何与周围环境交互的空间和功能知识，以及根据我们对当前目标的信念主动检索的与模式相关的事件集群。反过来，这些隐含的预测导致在层次结构的第一级序列中的下一个事件的概率自上而下预激活。

![图2](../Supporting_Information/2021-08-22-WM1-fig2.png)

# 5.使用分层生成模型来理解事件序列

## 5.1 关注事件模型

不知道事件主体的总体目标，是一种预期不确定性。贝叶斯推理的一个基本原则是，我们先前的预期不确定性越大，我们在遇到新的不可预测的输入（预期的惊喜）时更新我们的信念就越多。因此，在事件序列开始时，我们更新信念的速率会很高，使我们能够快速锁定生成我们观察到的事件序列的目标。

对目标的不确定性会产生较低层次的预激活影响。出现第一个事件后，会出现较大的自下而上的预测误差，进而到工作记忆层级。由工作记忆中的事件模型生成的越来越强的隐式预测，反过来，将在层次结构的第一级对即将到来的单个事件表示产生越来越强的自上而下的预激活。随着在第一级观察到每个新的一致传入事件，访问/检索它变得越来越容易（即，其处理变得容易），从而产生越来越小的第一级预测误差。

## 5.2 通过跟踪预测误差反应性地检测事件边界

- 预测误差必须有多大才能推断出事件边界的存在？

出现无法解释完整事件模型的事件后，会重新分配对主体目标的信念。这体现了贝叶斯推理的基本原理，称为贝叶斯奥卡姆剃刀：尽管我们试图尽可能简单地解释传入的数据，但如果我们遇到鉴于这些先前假设不太可能出现的新信息，我们将始终推断（检索或学习）为数据分配最高可能性的假设。

这体现了贝叶斯推理的基本原理，称为贝叶斯奥卡姆剃刀：尽管我们试图尽可能简单地解释传入的数据，但如果我们遇到鉴于这些先前假设不太可能出现的新信息，我们将始终推断（检索或学习）为数据分配最高可能性的假设。信念对目标的大规模重新分配也导致在生成层级较低层级的信念重新分配，以确保输入在整个模型中得到解释。在动态预测编码框架内，这种在生成层级最高层级的信念的追溯重新分配是由一个大的第二级自下而上的预测误差触发的，并且驱动层级较低层级的活动的追溯性重新分配通过追溯自上而下的反馈活动。

## 5.3通过跟踪不确定性主动检测事件边界

- 在没有非常大的预测误差的情况下是否可以推断出事件边界？我们如何检测这种不确定性的上升，并且在我们观察到即将到来的不可预测事件之前，我们是否能够利用它从我们当前的事件模型中脱离出来？

在框架中，会不断跟踪对主体目标预期的不确定性，当我们看到下一个不可预测的事件时，我们会准备好转变我们的高层信念，以便我们可以迅速找到新目标，这将促使她产生我们观察到的下一个事件序列。重要的是，由于我们对女性即将到来的目标的预期不确定性在实际观察到下一个意外事件之前开始上升，因此这种不确定性驱动的事件边界检测本质上是主动的。有人提出，我们随时间跟踪预期不确定性的能力提供了主动注意的规范说明。我们对感知流中即将到来的输入越不确定，我们在遇到这个新输入之前分配给环境的注意力就越多。

这种与预期不确定性相关的注意力的主动分配比与上述意外惊喜相关的注意力的反应性重新分配和反应性处理提供了几个优势。

1. 在预测编码框架内，当我们达到特定目标的最终状态时，不确定性的迅速上升将导致层次结构第二层自上而下的激活减少；它会引导我们在实际遇到下一个不可预测的事件之前主动脱离当前事件模型及其关联的与模式相关的集群。

2. 当我们达到目标最终状态时，对环境的预先分配为我们提供了另一个机会：我们可以在环境中积极寻找线索，告诉我们接下来要检索哪些与模式相关的集群，简称信息优化。在计算上，这被称为主动感知。在资源有限的系统中，比如大脑，主动感知为我们提供了一种从环境中收集信息的最佳方式，这些信息被认为在未来最有可能有用。

## 5.4 监测更广泛环境的动态：从意外到预期的惊喜

总而言之，这个概率层次生成框架提供了两种机制，我们可以通过这些机制在视觉事件实时顺序展开时推断边界。它还提供了有关我们如何能够使用这些信息脱离当前事件模型并在工作记忆中构建新事件模型的见解。

首先，正如事件分割理论所提出的，我们可以跟踪当每个传入事件改变当前事件模型的状态时产生的隐式预测误差的大小——事件模型的先验状态和新状态之间的差异。重要的是，这个概率框架为推断事件边界和切换到新事件模型的预测误差必须有多大提供了一个原则性的解释：即将到来的事件必须产生意想不到的惊喜——它必须不太可能超过我们先前的确定性我们认为正在生成当前事件模型的目标（潜在原因）。

在预测编码框架内，传入事件必须与工作内存中已经处于活动状态的信息发生冲突，从而产生第二级自下而上的预测错误，从而导致与当前事件模型的后期自上而下的追溯脱离和检索新的模式相关事件集群。

这两种机制是不同的。第一个是由大的预测错误（意料之外的惊喜）驱动的，本质上是反应性的，需要重新分配注意力。第二个是由不确定性驱动的，本质上是主动的，使我们能够预先注意环境输入，并为即将到来的事件边界预期的惊喜做好准备。如上所述，在这两种机制中，后者显然更可取：主动脱离当前事件模型以响应预期的不确定性上升比在意外发生后追溯脱离并“追赶”更有效。惊喜）。我们可以减少意外惊喜的机会并增加意外惊喜的机会的一种方法是监控我们环境的更广泛动态并相应地调整我们的“处理模式”。

主动的处理模式会有效分配资源，进而可以有更广泛的动态输入。

## 5.5 扩展生成层次结构

为了说明层次生成模型和预测编码的原理如何帮助我们理解事件理解的神经认知机制，我诉诸于一个相对简单的三层层次结构，其中目标直接生成事件集群，反过来，生成随时间顺序展开的单个事件。然而，在事件理解过程中，我们有时需要表示其他代理在更短或更长的时间跨度上的目标。例如，当我们看到厨房里的女人打开冰箱时，我们可以推断出她的短期子目标是{加牛奶到茶}。随着我们继续观察她的日常生活，我们可以推断她的长期目标是{做早餐}。

新的子目标层级：这个新级别将代表我们认为每个目标可能产生的一组可能和可能的子目标。每个子目标都会生成一系列较短的事件集群，每个集群都有不同的可能性。

![图3](../Supporting_Information/2021-08-22-WM1-fig3.png)

这种安排导致了一个树状的层次结构，其中任何给定的子目标都只会“看到”事件集群的一个子集，以及它下面的单个事件。但是，层次结构最高级别的目标将看到其下方的所有内容。这意味着在实时理解过程中，随着我们沿着一系列事件前进并且越来越接近推断总体目标，每个子目标将从这个目标中获得越来越多的自上而下的预激活。这将提供额外的自上而下的约束，进一步促进以下级别的处理。此外，每个子目标的最终状态的不确定性预期增加将为我们提供额外的灵活性，以响应意外输入过渡到新的子目标，这些新的子目标仍然通过来自更高级别目标的自上而下的激活通知.

关于下一个子目标的预期不确定性的小幅上升将导致我们稍微脱离对层次结构第一层的单个事件生成强大的自上而下的预测。遇到突发事件会更惊讶，更愿意灵活更新信念转向新的子目标。

生成模型的层次越高，其时间“接受域”就会越来越大。层次结构的较低级别将处理以更快速度变化的信息，允许在最终状态（从事件到事件，从子目标到子目标）的边界之间进行快速灵活的主动转换。层次结构的较高级别将处理变化较慢的信息，这既是因为推断新信息需要更长的时间，而且因为在这些更高级别推断的信息变得越来越难以改变。因此，在生成层次结构的更高级别上，任何预测误差的大小和跨边界转换所需的预期不确定性程度将逐渐变大。因此，在理解过程中，通常会在较低而不是较高的水平上解释未预测的传入事件。

## 5.6灵活的生成模型：理解者的目标

在实践中，这种对行动计划的“深度解构”通常是不必要的，最终，我们在理解过程中使用的生成模型的结构将是由我们自己的理解目标决定。

理解模式和深度差异很大，这取决于我们当前的“连贯性标准”。这些标准将取决于许多因素，包括我们的内在动机、我们正在执行的其他任务、我们更广泛环境的限制以及我们收到的任何指示。有时我们可能对其他主体的目标根本不感兴趣，而是用我们自己的一组正交目标和兴趣来理解。

事件理解并不总是等同于“反向”的事件产生。相反，我们使用的生成模型必须是高度动态和灵活的，允许我们推断他人的目标，但前提是这些信息与我们作为理解者实现自己的目标相关。

# 6.从事件理解到事件制作和学习

上述理解的分层生成框架的前提是（a）我们使用一组概率知识来描述我们对其他代理如何以及为什么产生事件的可概括假设——一个分层生成模型，以及（b）我们使用这个模型作为推理引擎来解释这些事件。这个框架的一个基本假设是，事件理解利用我们用来执行顺序动作的相同核心事件表示。

## 6.1 生成模型和顺序动作的产生

除了在事件理解期间简单地利用相同的潜在事件表示之外，顺序动作的产生还可能依赖于组装概率生成模型来主动预测即将到来的信息，就像在理解期间一样。连续动作必然要求我们不断地与环境互动。因此，与理解一样，在从自下而上的输入中预测来自环境的即将到来的信息应该提高处理的速度和效率。

通过使用生成模型来概率性地预先激活我们计划执行的目标、子目标和单个事件的最终状态，我们获得了领先优势。子目标【在茶里加牛奶】，最终状态【茶里的牛奶】的预期会触发执行下一事件【打开冰箱】，也可能会导致事件【牛奶在手里】这一结束状态的预激活。

在产生连续动作期间进行这种主动预测的证据来自研究表明，我们的眼睛在自然主义目标导向的任务中真正需要它们之前就注视它们。在这个主动生成的框架内，当我们对未来的目标非常确定时，我们不仅会预先激活与该目标相关的预期对象的视觉空间特征；我们还预先激活了我们计划行动的感知结果。

更具推测性的是，除了在预测即将到来的输入中发挥作用之外，我们在产生顺序行动期间使用的生成模型还可以跟踪我们自己的目标和子目标的不确定性，使我们能够“推断”我们自己行动计划的一部分基于环境输入。跟踪我们自己目标的预期确定性也可以让我们根据这些总体目标调整我们的“行动模式”。生成模型可能会在确保我们的行为不仅根据我们的环境而且根据我们自己的意图进行校准方面发挥作用，从而最大限度地减少生产中出现错误的机会——先发制人的错误监控。

综上所述，通过允许基于目标和子目标确定的主动预测和更新，顺序动作中的生成模型可能与事件理解中的生成模型起到类似的作用，从而确保处理的快速、准确和灵活.然而，同样重要的是要认识到我们参与理解和生产的生成模型将在重要方面有所不同。首先，理解者对生产者的目标几乎总是比生产者对他们自己的目标有更多的不确定性。其次，如第 5.6 节所述，我们在理解过程中使用的生成模型只会在这些表示与我们自己的目标一致的程度上代表其他智能体的可能目标和子目标。第三，在理解过程中，我们必须能够表示与模式相关的知识，这与我们在生产过程中实现特定目标时所使用的知识不同。

## 6.2 生成模型和学习

虽然我们永远无法完全访问彼此的大脑，但有一种重要的方式可以让我们的生成模型更紧密地结合在一起——我们可以通过在理解和采取行动的同时从环境中隐式学习来调整我们的模型它。越来越多的证据表明，学习与理解和生产密切相关，依赖于相同的计算算法。

在概率生成框架内，学习，就像理解和生产一样，需要使用贝叶斯规则来更新我们的信念。关键的区别在于，与其更新我们对已经学习的表征（推理）的信念，我们必须更新我们对生成模型本身参数的信念，这发生在更长的时间尺度上。作为理解者，我们总是对这些参数有一些不确定性，正是这种预期的不确定性驱使我们更新我们对这些参数的信念，同时更新我们对模型本身表示的信念。我们必须能够在长期记忆中编码和巩固这些信息。

我们如何能够从我们学到的信息中进行概括，以便我们知道哪些与模式相关的集群最适合在未来情况下检索。在所有这些情况下，我们需要能够推断出哪些类型的先前经验与当前情况最相关。

总的来说，这项工作强调了这样一个观点，即在这个概率生成框架内，理解、生产、学习和记忆是密切相关的。这当然是有道理的。理解、产生和从事件中学习不是独立的努力：当我们感知环境并对其采取行动时，这三个过程必须相互协作。通过参与概率预测，并跟踪我们先验信念的预测误差和不确定性，分层生成模型可以提供计算引擎，使我们能够利用我们周围世界的丰富统计结构并从中学习。

